{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "labeler-example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snpRGxi9rBIk"
      },
      "source": [
        "# Audacity WaveformToLabels Example\n",
        "\n",
        "In this notebook we will load in a [speech to text model](https://huggingface.co/facebook/s2t-medium-librispeech-asr) from Facebook using Huggingface's Transformers module/package. We will look at the necessary dependencies to serialize  a model, how to create a wrapper class for a pretrained WaveformToLabels model, and show how to save this wrapped model so that it can easily be used in Audacity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsxnQ50MOkV"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t_eER_7u03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42867602-1018-4a1f-bcb1-bad3e1c6e869"
      },
      "source": [
        "!pip install audacitorch\n",
        "!pip install torchaudio==0.9.0\n",
        "!pip install transformers"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: audacitorch in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from audacitorch) (2.6.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from audacitorch) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->audacitorch) (3.7.4.3)\n",
            "Collecting torchaudio==0.9.0\n",
            "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.9.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio==0.9.0) (3.7.4.3)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.9.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7c5hvQ978Cq"
      },
      "source": [
        "%%capture\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import torchaudio\n",
        "import json\n",
        "\n",
        "# use no grad!\n",
        "torch.set_grad_enabled(False)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTHwfStnMOkW"
      },
      "source": [
        "These packages will be needed if you want to upload your model to Huggingface using a CLI. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4q83Zsg_AlH"
      },
      "source": [
        "%%capture\n",
        "# required for huggingface\n",
        "!sudo apt-get install git-lfs\n",
        "!git lfs install"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eprLiH6w8Z_c"
      },
      "source": [
        "\n",
        "## Wraping the model\n",
        "We need to create a `.pt` containing the model itself, and a json string with the model's metadata. This meta data will tell end users about the model's domain, sample rate, labels, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlK3ZPGPMOkW"
      },
      "source": [
        "`torchaudacity` provides a [`WaveformToLabels` class](https://github.com/hugofloresgarcia/torchaudacity/blob/main/torchaudacity/core.py#L52). We will use this as a base class for our pretrained models wrapper. The `WaveformToLabels` class provides us with tests to ensure that our model is receiving properly sized input, and outputting the expected tensor shapes for Audacity's Deep Learning Analyzer, for a [graphical explination visit the main README here](https://github.com/hugofloresgarcia/torchaudacity#contributing-models-to-audacity). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzT-q31pMOkX"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJdsAR_uNEQ0"
      },
      "source": [
        "from audacitorch.core import WaveformToLabelsBase\n",
        "\n",
        "class sub_models(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", torchscript=True)\n",
        "        self._processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\", torchscript=True)\n",
        "        self.token_to_idx = {val:key for key, val in self._processor.tokenizer.decoder.items()}\n",
        "    \n",
        "\n",
        "class model_wrapper(WaveformToLabelsBase):\n",
        "    def do_forward_pass(self, _input):\n",
        "        input_values = self.model._processor(_input, return_tensors=\"pt\", padding=\"longest\").input_values[0]\n",
        "        logits = self.model._model(input_values)[0]\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcription = self.model._processor.decode(predicted_ids[0])   \n",
        "        num_preds = len(transcription)\n",
        "\n",
        "        # model predictions must be logits or one-hot encoded \n",
        "        preds_onehot = torch.FloatTensor(num_preds, len(self.model.token_to_idx))\n",
        "        preds_onehot.zero_()\n",
        "        for i, token in enumerate(transcription):\n",
        "            if token == ' ':\n",
        "                token = '<s>'\n",
        "            token_idx = self.model.token_to_idx[token]\n",
        "            preds_onehot[i][token_idx] = 0.99\n",
        "        \n",
        "        # this model does not use timestamps, therefore we will use \n",
        "        # equally sized time ranges for each prediction\n",
        "        total_time = _input.shape[1] / 16000\n",
        "        equal_size_timestamp = total_time / num_preds\n",
        "        timestamps = torch.FloatTensor(num_preds, 2)\n",
        "        timestamps.zero_()\n",
        "        for i in range(num_preds):\n",
        "            if i == 0:\n",
        "                timestamps[0][1] = equal_size_timestamp\n",
        "            else:\n",
        "                timestamps[i][0] = timestamps[i-1][1]\n",
        "                timestamps[i][1] = timestamps[i][0] + equal_size_timestamp\n",
        "\n",
        "        # return the predictions and timestamps as a tensor\n",
        "        preds = torch.argmax(preds_onehot, dim=-1, keepdim=False)       \n",
        "        print(preds) \n",
        "        return (preds, timestamps)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "8zBJFmN5MOkY"
      },
      "source": [
        "sub_models = sub_models()\n",
        "torchscript_model = model_wrapper(sub_models)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Nl_IuaChYu"
      },
      "source": [
        "## Model Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSpBRRjZCj1U"
      },
      "source": [
        "We need to create a `metadata.json` file for our model. This file will be added to the Huggingface repo and will provide Audacity with important information about our model. This allows for users to quickly get important information about this model directly from Audacity. See the [contributing documentation](https://github.com/hugofloresgarcia/torchaudacity) for the full metadata schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqWLNGtAMOkY"
      },
      "source": [
        "vocab = [str(letter) for letter in sub_models._processor.tokenizer.decoder.values()]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGJaAX7QCd4W"
      },
      "source": [
        "# create a dictionary with model metadata\n",
        "metadata = {\n",
        "    'sample_rate': 16000, \n",
        "    'domain_tags': ['speech'],\n",
        "    'short_description': 'I will label your speech into text :]',\n",
        "    'long_description': \n",
        "              'This is an Audacity wrapper for the model, '\n",
        "              'forked from the repository '\n",
        "              'facebook/wav2vec2-base-960h'\n",
        "              'This model was trained by Alexei Baevski'\n",
        "              ', Henry Zhou, Abdelrahman Mohamed and,'\n",
        "              'Michael Auli.',\n",
        "    'tags': ['speech-to-text'],\n",
        "    'effect_type': 'waveform-to-labels',\n",
        "    'multichannel': False,\n",
        "    'labels': vocab,\n",
        "}"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBGzAPpgMOkY"
      },
      "source": [
        "## Saving Our Model & Metadata\n",
        "\n",
        "We will now save the wrapped model locally by tracing it with torchscript, and generating a `ScriptModule` or `ScriptFunction` using `torch.jit.script`. We can then use `torchaudacity's` utility function `save_model` to save the model and meta data easily. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HKQspnf_hJM"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from audacitorch.utils import save_model, get_example_inputs\n",
        "import torchaudio "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In-r4gJlE-H9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "outputId": "521346fb-e93f-4b08-d280-0f6931943364"
      },
      "source": [
        "# compiling and saving model\n",
        "# dummy_input = torch.randn((1, 2048)) # dummy input for model tracing\n",
        "# waveform, sample_rate = torchaudio.load(os.path.join(os.getcwd(), 'assets/test.wav')) # test audio for tracing\n",
        "example_inputs = get_example_inputs()\n",
        "print(torchscript_model(example_inputs[0]))\n",
        "traced_model = torch.jit.trace(torchscript_model, example_inputs[0], check_inputs=example_inputs)\n",
        "\n",
        "# serialized_model = torch.jit.script(torchscript_model)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/audacitorch/core.py:7: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert x.shape[-1] > x.shape[0], f\"The number of channels {x.shape[-2]} exceeds the number of samples {x.shape[-1]} in your INPUT waveform. \\\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py:1966: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return obj.detach().cpu().numpy()\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py:158: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  tensor = as_tensor(value)\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/feature_extraction_wav2vec2.py:206: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
            "  padded_inputs[\"input_values\"] = [np.asarray(array, dtype=np.float32) for array in input_values]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:538: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:575: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py:1946: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  return obj.detach().cpu().tolist()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "/usr/local/lib/python3.7/dist-packages/audacitorch/core.py:68: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert labels.shape[0] == timestamps.shape[0], \"time dimension between \"\\\n",
            "/usr/local/lib/python3.7/dist-packages/audacitorch/core.py:70: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert timestamps.shape[1] == 2, \"second dimension of the timestamps tensor\"\\\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d86b848ec9e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_example_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchscript_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtraced_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorchscript_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# serialized_model = torch.jit.script(torchscript_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0m_module_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         )\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m                 \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m                 \u001b[0margument_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m             )\n\u001b[1;32m    961\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The traced function didn't return any values! Side-effects are not captured in traces, so it would be a no-op."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlNh0QEVMOkZ"
      },
      "source": [
        "save_model(traced_model, metadata, Path('audacity-Wav2Vec2-Base'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMXnAbSRMOkZ"
      },
      "source": [
        "## Upload your model\n",
        "Now you're ready to upload your model, in the case of this note book the model is stored in a folder titled 'audacity-s2t-medium'. For more information see [the main README](https://github.com/hugofloresgarcia/torchaudacity#exporting-to-huggingface) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEb572oCMOkZ"
      },
      "source": [
        "--- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DESrwhNfMOkZ"
      },
      "source": [
        "## Note on Huggingface `transformers` module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hydDsCBMOkZ"
      },
      "source": [
        "Currently the Huggingface `transformers` module has limited support when exporting a model to torchscript. Through trial and error we have found that the [`Wav2Vec2`](https://huggingface.co/transformers/model_doc/wav2vec2.html) models seem to export with little issue. The [`Speech2Text`](https://huggingface.co/transformers/model_doc/speech_to_text.html) models appear to have issues when exported to torchscript. \n",
        "\n",
        "For more information about the Huggingface `transformers` torchscript compatiliablity follow the [this hyperlink](https://huggingface.co/transformers/torchscript.html). "
      ]
    }
  ]
}