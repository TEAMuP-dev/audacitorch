{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "labeler-example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "interpreter": {
      "hash": "a71058fb1b1fb68e1a188f0be12c1742382ea541f6b598b12f61a6704df9ab6f"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snpRGxi9rBIk"
      },
      "source": [
        "# Audacity WaveformToLabels Example\n",
        "\n",
        "In this notebook we will load in a [speech to text model](https://huggingface.co/facebook/s2t-medium-librispeech-asr) from Facebook using Huggingface's Transformers module/package. We will look at the necessary dependencies to serialize  a model, how to create a wrapper class for a pretrained WaveformToLabels model, and show how to save this wrapped model so that it can easily be used in Audacity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHbwCBvhX0GA"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t_eER_7u03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f8ab03-659b-42d6-c894-61ac46b003ed"
      },
      "source": [
        "!pip install torchaudio==0.9.0\n",
        "!pip install transformers\n",
        "!pip install audacitorch"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio==0.9.0) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchaudio==0.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Collecting audacitorch\n",
            "  Downloading audacitorch-0.0.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from audacitorch) (2.6.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from audacitorch) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->audacitorch) (3.7.4.3)\n",
            "Installing collected packages: audacitorch\n",
            "Successfully installed audacitorch-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7c5hvQ978Cq"
      },
      "source": [
        "%%capture\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import torchaudio\n",
        "import json\n",
        "\n",
        "# use no grad!\n",
        "torch.set_grad_enabled(False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ7fK5pNX0GB"
      },
      "source": [
        "These packages will be needed if you want to upload your model to Huggingface using a CLI. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4q83Zsg_AlH"
      },
      "source": [
        "# %%capture\n",
        "# # required for huggingface\n",
        "# !sudo apt-get install git-lfs\n",
        "# !git lfs install"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eprLiH6w8Z_c"
      },
      "source": [
        "\n",
        "## Wrapping the model\n",
        "We need to create a `.pt` containing the model itself, and a json string with the model's metadata. This meta data will tell end users about the model's domain, sample rate, labels, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDsKJJ0EX0GC"
      },
      "source": [
        "`torchaudacity` provides a [`WaveformToLabels` class](https://github.com/hugofloresgarcia/torchaudacity/blob/main/torchaudacity/core.py#L52). We will use this as a base class for our pretrained models wrapper. The `WaveformToLabels` class provides us with tests to ensure that our model is receiving properly sized input, and outputting the expected tensor shapes for Audacity's Deep Learning Analyzer, for a [graphical explination visit the main README here](https://github.com/hugofloresgarcia/torchaudacity#contributing-models-to-audacity). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXjWcrWGX0GD"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJdsAR_uNEQ0"
      },
      "source": [
        "from audacitorch.core import WaveformToLabelsBase\n",
        "\n",
        "class SubModels(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", torchscript=True)\n",
        "        self._processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\", torchscript=True)\n",
        "        self.token_to_idx = {val:key for key, val in self._processor.tokenizer.decoder.items()}\n",
        "    \n",
        "\n",
        "@torch.jit.script_if_tracing\n",
        "def get_timestamps(num_preds: int, total_time: int):\n",
        "  \"\"\" if the model produces no output, prevent a division by zero error\"\"\"\n",
        "  if num_preds == 0:\n",
        "    return torch.empty(1, 0)\n",
        "  else:\n",
        "    equal_size_timestamp = total_time / num_preds\n",
        "    timestamps = torch.zeros(num_preds, 2)\n",
        "    return timestamps\n",
        "\n",
        "@torch.jit.script_if_tracing\n",
        "def check_empty_output(preds, timestamps):\n",
        "  \"\"\" we need to create fake output if our model produces empty output\"\"\"\n",
        "  if preds.shape[0] == 0:\n",
        "    return torch.tensor([0]), torch.tensor([[0., 0.01]])\n",
        "  else:\n",
        "    return preds, timestamps\n",
        "\n",
        "\n",
        "class ModelWrapper(WaveformToLabelsBase):\n",
        "    def do_forward_pass(self, _input):\n",
        "        input_values = self.model._processor(_input, return_tensors=\"pt\", padding=\"longest\").input_values[0]\n",
        "        logits = self.model._model(input_values)[0]\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        transcription = self.model._processor.decode(predicted_ids[0])   \n",
        "        num_preds = len(transcription)\n",
        "\n",
        "        # model predictions must be logits or one-hot encoded \n",
        "        preds_onehot = torch.FloatTensor(num_preds, len(self.model.token_to_idx))\n",
        "        preds_onehot.zero_()\n",
        "        for i, token in enumerate(transcription):\n",
        "            if token == ' ':\n",
        "                token = '<s>'\n",
        "            token_idx = self.model.token_to_idx[token]\n",
        "            preds_onehot[i][token_idx] = 0.99\n",
        "        \n",
        "        # this model does not use timestamps, therefore we will use \n",
        "        # equally sized time ranges for each prediction\n",
        "        total_time = _input.shape[1] / 16000\n",
        "        \n",
        "        timestamps = get_timestamps(num_preds, total_time)\n",
        "        for i in range(num_preds):\n",
        "            if i == 0:\n",
        "                timestamps[0][1] = equal_size_timestamp\n",
        "            else:\n",
        "                timestamps[i][0] = timestamps[i-1][1]\n",
        "                timestamps[i][1] = timestamps[i][0] + equal_size_timestamp\n",
        "\n",
        "        # return the predictions and timestamps as a tensor\n",
        "        preds = torch.argmax(preds_onehot, dim=-1, keepdim=False)       \n",
        "        preds, timestamps = check_empty_output(preds, timestamps)\n",
        "        print(preds, timestamps)\n",
        "        return (preds, timestamps)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "MTTX7rSUX0GE"
      },
      "source": [
        "sub_models = SubModels()\n",
        "torchscript_model = ModelWrapper(sub_models)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25Nl_IuaChYu"
      },
      "source": [
        "## Model Metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSpBRRjZCj1U"
      },
      "source": [
        "We need to create a `metadata.json` file for our model. This file will be added to the Huggingface repo and will provide Audacity with important information about our model. This allows for users to quickly get important information about this model directly from Audacity. See the [contributing documentation](https://github.com/hugofloresgarcia/torchaudacity) for the full metadata schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvTSiqyOX0GF"
      },
      "source": [
        "vocab = [str(letter) for letter in sub_models._processor.tokenizer.decoder.values()]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGJaAX7QCd4W"
      },
      "source": [
        "# create a dictionary with model metadata\n",
        "metadata = {\n",
        "    'sample_rate': 16000, \n",
        "    'domain_tags': ['speech'],\n",
        "    'short_description': 'I will label your speech into text :]',\n",
        "    'long_description': \n",
        "              'This is an Audacity wrapper for the model, '\n",
        "              'forked from the repository '\n",
        "              'facebook/wav2vec2-base-960h'\n",
        "              'This model was trained by Alexei Baevski'\n",
        "              ', Henry Zhou, Abdelrahman Mohamed and,'\n",
        "              'Michael Auli.',\n",
        "    'tags': ['speech-to-text'],\n",
        "    'effect_type': 'waveform-to-labels',\n",
        "    'multichannel': False,\n",
        "    'labels': vocab,\n",
        "}"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBX7eWGlX0GF"
      },
      "source": [
        "## Saving Our Model & Metadata\n",
        "\n",
        "We will now save the wrapped model locally by tracing it with torchscript, and generating a `ScriptModule` or `ScriptFunction` using `torch.jit.script`. We can then use `torchaudacity's` utility function `save_model` to save the model and meta data easily. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HKQspnf_hJM"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from audacitorch.utils import save_model, get_example_inputs\n",
        "import torchaudio "
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In-r4gJlE-H9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00640aae-e239-448e-c6e0-c07ecf443c17"
      },
      "source": [
        "# compiling and saving model\n",
        "example_inputs = get_example_inputs()\n",
        "traced_model = torch.jit.trace(torchscript_model, example_inputs[0])"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0]) tensor([[0.0000, 0.0100]])\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh6ebZSacC1n"
      },
      "source": [
        "WaveformToLabelsBase?"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "zgI_0Um3X0GF",
        "outputId": "54464329-fcd5-46e3-cefc-6090d1d78897"
      },
      "source": [
        "save_model(traced_model, metadata, Path('audacity-Wav2Vec2-Base'))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-3c312e862f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraced_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'audacity-Wav2Vec2-Base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'traced_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwIqakexX0GF"
      },
      "source": [
        "## Upload your model\n",
        "Now you're ready to upload your model, in the case of this note book the model is stored in a folder titled 'audacity-s2t-medium'. For more information see [the main README](https://github.com/hugofloresgarcia/torchaudacity#exporting-to-huggingface) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHssJP49X0GG"
      },
      "source": [
        "--- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obbyj4EyX0GG"
      },
      "source": [
        "## Note on Huggingface `transformers` module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBQyMXEKX0GG"
      },
      "source": [
        "Currently the Huggingface `transformers` module has limited support when exporting a model to torchscript. Through trial and error we have found that the [`Wav2Vec2`](https://huggingface.co/transformers/model_doc/wav2vec2.html) models seem to export with little issue. The [`Speech2Text`](https://huggingface.co/transformers/model_doc/speech_to_text.html) models appear to have issues when exported to torchscript. \n",
        "\n",
        "For more information about the Huggingface `transformers` torchscript compatiliablity follow the [this hyperlink](https://huggingface.co/transformers/torchscript.html). "
      ]
    }
  ]
}